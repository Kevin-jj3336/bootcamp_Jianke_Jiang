{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment & Config Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env loaded (if present)\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import re\n",
    "import io\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "from src.config import load_env, get_key\n",
    "load_env()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API_KEY present: True\n"
     ]
    }
   ],
   "source": [
    "api_key = get_key(\"API_KEY\"); print(f\"API_KEY present: {api_key is not None}\")\n",
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "RAW = pathlib.Path(\"..\") / \"data/raw\"; RAW.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts():\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "def save_csv(df: pd.DataFrame, prefix: str, **meta):\n",
    "    mid = '_'.join([f\"{k}-{v}\" for k,v in meta.items()])\n",
    "    path = RAW / f\"{prefix}_{mid}_{ts()}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print('Saved', path)\n",
    "    return path\n",
    "\n",
    "def validate(df: pd.DataFrame, required):\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    return {'missing': missing, 'shape': df.shape, 'na_total': int(df.isna().sum().sum())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage2 - Data Acquisition - API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##     - Data in Real-time Availability - Need 10 mintues level update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citi Bike Station Status - real-time availability\n",
    "url = \"https://gbfs.citibikenyc.com/gbfs/en/station_status.json\"\n",
    "resp = requests.get(url)\n",
    "js = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../data/raw/api_source-alpha_symbol-Station Status_20250820-202642.csv\n"
     ]
    }
   ],
   "source": [
    "data = js[\"data\"][\"stations\"]\n",
    "df_api = pd.DataFrame(data)\n",
    "df_api = df_api.reset_index()\n",
    "v_api = validate(df_api, ['date','close']); v_api\n",
    "_ = save_csv(df_api, prefix='api', source='alpha' , symbol=\"Station Status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---Data Structure\n",
    "{\"data\":{\"Station\":[{Station1},{Station2},{Station3}]},\"\"last_updated\": 1755700740, \"ttl\": 60, \"version\": \"1.1\"}\n",
    "\n",
    "### ---Inside {Station1}\n",
    "\t•\tstation_id: \n",
    "\t•\tlegacy_id: \n",
    "            # ID used in early stage to merge with old system\n",
    "\t•\tis_installed:  \n",
    "            # Is station physically installed? 1: yes 0: No\n",
    "\t•\tis_renting: \n",
    "            # Is renting Allowed?\n",
    "\t•\tis_returning: \n",
    "            # Is returning Allowed?\n",
    "\t•\tnum_bikes_available: \n",
    "\t•\tnum_ebikes_available: \n",
    "            # How many avaliable electric bike\n",
    "\t•\tnum_bikes_disabled: \n",
    "            # How many bike not operating\n",
    "\t•\tnum_docks_available: \n",
    "\t•\tnum_docks_disabled: \n",
    "            # How many docks not operating\n",
    "\t•\teightd_has_available_keys: \n",
    "            # keys for VIP avaliable?\n",
    "\t•\tlast_reported: 1754914801\n",
    "            # use pd.to_datetime(..., unit=\"s\") to convert to readable time\n",
    "\n",
    "## Documentation\n",
    "- API Source: \n",
    "    https://gbfs.citibikenyc.com/gbfs/en/station_status.json \n",
    "    •\tKey parameters:\n",
    "\t    •symbol=AAPL\n",
    "\t    •outputsize=compact\n",
    "\t    •apikey=${ALPHAVANTAGE_API_KEY}（from .env）\n",
    "- Scrape Source: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
    "- Assumptions & risks: \n",
    "    1:API rate limit: Alpha Vantage allow free user 5 request per mintue and 500 per day, frequent access would be banned from this API\n",
    "    2:For parsing, the wikipidia's table structure may change, which may break parsing\n",
    "    3: schema changes, which is backed up by the validation funciton\n",
    "    4: Assuming the data source only has tolarable data completeness and consistency\n",
    "- Confirm `.env` is not committed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Rider Data\n",
    "#    -data is tremendously big, I will only use the last three month data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备处理 3 个 ZIP…\n",
      "[1/3] 下载 https://s3.amazonaws.com/tripdata/202505-citibike-tripdata.zip\n",
      " 提取 202505-citibike-tripdata_4.csv -> extracted/202505-citibike-tripdata__202505-citibike-tripdata_4.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202505-citibike-tripdata_5.csv -> extracted/202505-citibike-tripdata__202505-citibike-tripdata_5.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202505-citibike-tripdata_1.csv -> extracted/202505-citibike-tripdata__202505-citibike-tripdata_1.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202505-citibike-tripdata_2.csv -> extracted/202505-citibike-tripdata__202505-citibike-tripdata_2.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202505-citibike-tripdata_3.csv -> extracted/202505-citibike-tripdata__202505-citibike-tripdata_3.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      "All Done!\n",
      "总汇总文件：../data/raw/citibike_all_trips20250820-203446.csv\n",
      "逐表备份目录：extracted/\n",
      "[2/3] 下载 https://s3.amazonaws.com/tripdata/202506-citibike-tripdata.zip\n",
      " 提取 202506-citibike-tripdata_2.csv -> extracted/202506-citibike-tripdata__202506-citibike-tripdata_2.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202506-citibike-tripdata_3.csv -> extracted/202506-citibike-tripdata__202506-citibike-tripdata_3.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202506-citibike-tripdata_1.csv -> extracted/202506-citibike-tripdata__202506-citibike-tripdata_1.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202506-citibike-tripdata_4.csv -> extracted/202506-citibike-tripdata__202506-citibike-tripdata_4.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202506-citibike-tripdata_5.csv -> extracted/202506-citibike-tripdata__202506-citibike-tripdata_5.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      "All Done!\n",
      "总汇总文件：../data/raw/citibike_all_trips20250820-203446.csv\n",
      "逐表备份目录：extracted/\n",
      "[3/3] 下载 https://s3.amazonaws.com/tripdata/202507-citibike-tripdata.zip\n",
      " 提取 202507-citibike-tripdata_5.csv -> extracted/202507-citibike-tripdata__202507-citibike-tripdata_5.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202507-citibike-tripdata_4.csv -> extracted/202507-citibike-tripdata__202507-citibike-tripdata_4.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202507-citibike-tripdata_1.csv -> extracted/202507-citibike-tripdata__202507-citibike-tripdata_1.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202507-citibike-tripdata_3.csv -> extracted/202507-citibike-tripdata__202507-citibike-tripdata_3.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      " 提取 202507-citibike-tripdata_2.csv -> extracted/202507-citibike-tripdata__202507-citibike-tripdata_2.csv.csv ；已追加到 ../data/raw/citibike_all_trips20250820-203446.csv\n",
      "All Done!\n",
      "总汇总文件：../data/raw/citibike_all_trips20250820-203446.csv\n",
      "逐表备份目录：extracted/\n"
     ]
    }
   ],
   "source": [
    "# ===== 路径设置：把输出放到 notebook 的上一级目录中的另一个文件夹 =====\n",
    "NOTEBOOK_DIR = pathlib.Path.cwd()      \n",
    "PARENT_DIR = NOTEBOOK_DIR.parent            # 母文件夹\n",
    "TARGET_DIR = PARENT_DIR / \"data\"/\"raw\"  # ← 你想要的“另一个文件夹”名称，改成你喜欢的\n",
    "EXTRACT_DIR = TARGET_DIR / \"extracted\"        # 子文件夹：单表导出\n",
    "OUTPUT_CSV = RAW / f\"citibike_all_trips{ts()}.csv\"\n",
    "\n",
    "# ========== 你可以在这里设定过滤规则 ==========\n",
    "YEARS = None          # 例：{2022, 2023, 2024}；为 None 表示不过滤\n",
    "MAX_ZIPS = None       # 例：先测试只跑 5 个；为 None 表示不限\n",
    "EXTRACT_DIR = \"extracted\"\n",
    "TIMEOUT = 60\n",
    "RETRIES = 3\n",
    "LAST_N_MONTHS = 3           # 要抓取的最近 N 个月\n",
    "INCLUDE_CURRENT_MONTH = False  # 是否把“当月”也算进去\n",
    "INDEX_URL = \"https://s3.amazonaws.com/tripdata\"\n",
    "# ============================================\n",
    "\n",
    "def target_yearmonths(n=LAST_N_MONTHS, include_current=INCLUDE_CURRENT_MONTH):\n",
    "    \"\"\"返回形如 {'202507','202506','202505'} 的集合\"\"\"\n",
    "    today = date.today()\n",
    "    # 基准月：含当月则从当月开始；否则从上个月开始\n",
    "    start_month = today if include_current else (today.replace(day=1) - relativedelta(days=1))\n",
    "    cur = start_month.replace(day=1)  # 取每月 1 号方便回溯整月\n",
    "    yms = []\n",
    "    for _ in range(n):\n",
    "        yms.append(f\"{cur.year}{cur.month:02d}\")\n",
    "        cur = (cur.replace(day=1) - relativedelta(days=1)).replace(day=1)\n",
    "    return set(yms)\n",
    "\n",
    "def list_zip_urls(index_url=INDEX_URL):\n",
    "    resp = requests.get(index_url, timeout=TIMEOUT)\n",
    "    resp.raise_for_status()\n",
    "    text = resp.text.strip()\n",
    "    hrefs = []\n",
    "\n",
    "    soup = BeautifulSoup(text, \"xml\")\n",
    "    for loc in soup.find_all(\"Key\"):\n",
    "        key = loc.get_text().strip()\n",
    "        if key.lower().endswith(\".zip\"):\n",
    "            hrefs.append(urljoin(index_url + \"/\", key))\n",
    "\n",
    "    hrefs = sorted(set(hrefs))\n",
    "\n",
    "    # ===== 新增：仅保留形如 202411-citibike-tripdata.zip 的“月包”，并过滤到最近 N 个月 =====\n",
    "    ym_set = target_yearmonths()\n",
    "    month_zip_pattern = re.compile(r'/(\\d{6})-citibike-tripdata\\.zip$', re.IGNORECASE) #learned from GPT that this way, I can get the 6 digit code for month\n",
    "\n",
    "    filtered = []\n",
    "    for u in hrefs:\n",
    "        # See if the URL matches with start with 6 digit time code with \"-citibike-tripdata.zip\"\n",
    "        m = month_zip_pattern.search(u)\n",
    "        if not m:\n",
    "            continue  # 排除形如 2013-citibike-tripdata.zip 的“年包”或其他命名\n",
    "        ym = m.group(1) #提取第一个捕获组，which is the 6 digit code\n",
    "        if ym in ym_set:\n",
    "            filtered.append(u)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "# Goal of this function is to download the file from one URL write it to a buffer\n",
    "def robust_get(url, timeout=TIMEOUT, retries=RETRIES, backoff=2.0):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=timeout) as r:\n",
    "                r.raise_for_status() #learned from GPT that this is a good way to handle errors\n",
    "                                     # When we get 404 not found, it will raise an HTTPError, if not this code, then it will not\n",
    "                                     # Some expected common status include:\t  •\t200 = success\n",
    "                                                                            # •\t404 = Not Found（找不到文件）\n",
    "                                                                            # •\t403 = Forbidden（权限问题）\n",
    "                                                                            # •\t500 = Internal Server Error（服务器错误）\n",
    "                buf = io.BytesIO() #learned from GPT that this is a good way to handle large files(create buffer)\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 1024): #iterate 1MB by 1MB\n",
    "                    if chunk:\n",
    "                        buf.write(chunk) #Write it in to the buffer\n",
    "                buf.seek(0) # Reset the buffer position to the beginning， so we can read\n",
    "                return buf\n",
    "        except Exception as e:\n",
    "            if i == retries - 1:\n",
    "                raise\n",
    "            time.sleep(backoff ** i)\n",
    "\n",
    "def iter_tables_from_zip(zip_bytes, zip_name_hint=\"\"):\n",
    "    \"\"\"\n",
    "    逐个返回 (member_name, df)。CSV/Excel 都支持。\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_bytes) as zf: # learned from GPT that this is a good way to handle zip files，similar to \"=\" but do not need to close\n",
    "        for member in zf.infolist(): #list all file and folder of the zip file\n",
    "            name = member.filename\n",
    "            lower = name.lower()\n",
    "            # 忽略目录和非表格文件\n",
    "            if member.is_dir():\n",
    "                continue\n",
    "            try:\n",
    "                with zf.open(member) as f: #open the member of the zip file\n",
    "                    raw = f.read()\n",
    "                bio = io.BytesIO(raw) # create a buffer to read the file\n",
    "                if lower.endswith(\".csv\"): #see if the file name ends with .csv, as we only need the csv file\n",
    "                    df = pd.read_csv(bio, low_memory=False)\n",
    "                elif lower.endswith(\".xlsx\") or lower.endswith(\".xls\"): #just incase if it is xls or xlsx\n",
    "                    df = pd.read_excel(bio)\n",
    "                else:\n",
    "                    #not in our interest, so just skip\n",
    "                    continue\n",
    "                yield name, df\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] 读取 {zip_name_hint}:{name} 失败 -> {e}\")\n",
    "                continue\n",
    "\n",
    "#make the filename safe for the file system\n",
    "def sanitize_filename(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', s)\n",
    "\n",
    "# Append all the data to one master CSV file, aligning columns by union\n",
    "def append_to_master_csv(df: pd.DataFrame, csv_path: str):\n",
    "    # 用列并集对齐（可能各年度列名不同）\n",
    "    if not os.path.exists(csv_path):\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return\n",
    "\n",
    "    # If the file already exists, read its header to align columns\n",
    "    header = pd.read_csv(csv_path, nrows=0)\n",
    "    all_cols = sorted(set(header.columns) | set(df.columns)) #把旧文件和新文件的列名并集\n",
    "    df2 = df.reindex(columns=all_cols) #保证新数据的列顺序以及空间意义的位置一致和旧文件一致 - this is suggested by GPT during its review on my code, while not nessarily needed in my context, I think this can be added so I can use it in future context\n",
    "\n",
    "    # CSV may also have different columns, so we need to ensure the union of columns\n",
    "    if set(all_cols) != set(header.columns):\n",
    "        # 需要把旧 CSV 重写为列并集（可能较慢，但稳妥）\n",
    "        tmp = pd.read_csv(csv_path)\n",
    "        tmp2 = tmp.reindex(columns=all_cols)\n",
    "        tmp2.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    \n",
    "    # Now we write the new data to the master CSV\n",
    "    df2.to_csv(csv_path, mode=\"a\", index=False, header=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "urls = list_zip_urls()\n",
    "print(f\"准备处理 {len(urls)} 个 ZIP…\")\n",
    "\n",
    "for i, url in enumerate(urls, 1):\n",
    "    print(f\"[{i}/{len(urls)}] 下载 {url}\")\n",
    "    zip_bytes = robust_get(url)\n",
    "    zip_base = os.path.basename(urlparse(url).path)\n",
    "    # By search online,\n",
    "    # find, urlparse(url).path gives path of url \"https://data.citybik.es/data/202301/202301-citibike-tripdata.zip?token=abc\" \n",
    "                                                                                #gives \"/data/202301/202301-citibike-tripdata.zip\"\n",
    "    # os.path.basename() gives the last part of the path, which is \"202301-citibike-tripdata.zip?token=abc\"\n",
    "    for member_name, df in iter_tables_from_zip(zip_bytes, zip_name_hint=zip_base):\n",
    "        # 1) 单表另存（方便校验）\n",
    "        safe_name = sanitize_filename(f\"{os.path.splitext(zip_base)[0]}__{os.path.basename(member_name)}\")\n",
    "        out_path = os.path.join(EXTRACT_DIR, f\"{safe_name}.csv\")\n",
    "        df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "        # 2) 统一合并到一个总 CSV（按列并集对齐）\n",
    "        append_to_master_csv(df, OUTPUT_CSV)\n",
    "        print(f\" 提取 {member_name} -> {out_path} ；已追加到 {OUTPUT_CSV}\")\n",
    "\n",
    "    print(\"All Done!\")\n",
    "    print(f\"总汇总文件：{OUTPUT_CSV}\")\n",
    "    print(f\"逐表备份目录：{EXTRACT_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "- API Source: \n",
    "    https://gbfs.citibikenyc.com/gbfs/en/station_status.json\n",
    "\n",
    "- Scrape Source: https://s3.amazonaws.com/tripdata\n",
    "- Assumptions & risks: \n",
    "    1: API rate limit: the API is currently free and public, however, this may not be the case in the future\n",
    "    2: For parsing, since there are multiple zip files on the website, and our methodology uses how the zip file is named to find our data. If newer files format changed, the code cannot parse the correct file from the website\n",
    "    3: the size of the data is huge, there is a hardware requirement running this program\n",
    "    4: Assuming the data source only has tolarable data completeness and consistency\n",
    "- Confirm `.env` is not committed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bootcamp_env]",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
