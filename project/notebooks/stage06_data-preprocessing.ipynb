{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stage06_data-preprocessing\n",
    "\n",
    " - filter the none needed data\n",
    " - filling all the NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os, pathlib\n",
    "\n",
    "parent_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(parent_dir)\n",
    "from src import cleaning\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Dataset - citibike_all_trips\n",
    "- I want to achieve auto load, even I reload the data in the raw file with different time name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read csv: ../data/raw/citibike_all_trips20250820-203446.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7d/x7l36_js44n_hzx_nffjmj440000gn/T/ipykernel_8523/2892501515.py:10: DtypeWarning: Columns (0,1,2,5,7,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(latest_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ride_id  rideable_type               started_at  \\\n",
      "0  7FFC2F964DC9F335  electric_bike  2025-05-28 07:25:55.506   \n",
      "1  EF51D1DA2B8F391C  electric_bike  2025-05-16 22:47:38.278   \n",
      "2  3065F3AABFC64C5D  electric_bike  2025-05-25 15:52:48.010   \n",
      "3  17F1785E42AF0DFE  electric_bike  2025-05-29 08:29:18.717   \n",
      "4  4F9FA463022AAD49   classic_bike  2025-05-15 18:37:16.877   \n",
      "\n",
      "                  ended_at               start_station_name start_station_id  \\\n",
      "0  2025-05-28 07:32:06.123                   E 7 St & Ave C          5545.01   \n",
      "1  2025-05-16 23:07:01.352                  9 Ave & W 39 St          6644.08   \n",
      "2  2025-05-25 16:03:29.522        Central Park W & W 103 St          7577.27   \n",
      "3  2025-05-29 08:54:24.022  Pulaski St & Marcus Garvey Blvd          4656.03   \n",
      "4  2025-05-15 18:45:37.416         Lafayette St & Jersey St          5561.06   \n",
      "\n",
      "            end_station_name end_station_id  start_lat  start_lng    end_lat  \\\n",
      "0    Bleecker St & Crosby St        5679.08  40.724129 -73.979013  40.726156   \n",
      "1    Amsterdam Ave & W 82 St         7360.1  40.756404 -73.994101  40.785247   \n",
      "2    Amsterdam Ave & W 82 St         7360.1  40.795590 -73.961884  40.785247   \n",
      "3  Lincoln Pl & Nostrand Ave        3952.07  40.693398 -73.939877   40.67077   \n",
      "4            W 20 St & 8 Ave        6224.05  40.724561 -73.995653  40.743453   \n",
      "\n",
      "     end_lng member_casual  \n",
      "0 -73.995102        member  \n",
      "1 -73.976673        casual  \n",
      "2 -73.976673        member  \n",
      "3   -73.9507        casual  \n",
      "4  -74.00004        member  \n"
     ]
    }
   ],
   "source": [
    "folder = \"../data/raw\"\n",
    "prefix = \"citibike_all_trips\"\n",
    "\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if f.startswith(prefix)]\n",
    "\n",
    "# we want the latest file that was updated that is started with the prefix\n",
    "latest_file = max(files, key=os.path.getmtime)\n",
    "print(\"read csv:\", latest_file)\n",
    "\n",
    "df = pd.read_csv(latest_file)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the data, throw the one we do not need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Help to drop all NaN values\n",
    "# Since this is a system data, I am assuming it is missing completely at Random (MCAR)\n",
    "df = cleaning.drop_missing(df, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, number of trips in Brooklyn: 301051\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from shapely.geometry import shape\n",
    "import requests\n",
    "\n",
    "# --- 0) 清洗经纬度列：统一成 float ---\n",
    "for col in [\"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\"]:\n",
    "    # 去除空格再转数值；失败的记为 NaN\n",
    "    df[col] = pd.to_numeric(df[col].astype(str).str.strip(), errors=\"coerce\")\n",
    "\n",
    "# 可选：粗过滤数值范围（避免把 400.0 之类的脏数据带进来）\n",
    "# As the approcimate long and lat of NYC is：lat [40, 41]，lng [-75, -72]\n",
    "# Mega Filter out the obviously wrong data\n",
    "valid = (\n",
    "    df[\"start_lat\"].between(40, 41) & df[\"start_lng\"].between(-75, -72) &\n",
    "    df[\"end_lat\"].between(40, 41)   & df[\"end_lng\"].between(-75, -72)\n",
    ")\n",
    "df = df[valid].reset_index(drop=True)\n",
    "\n",
    "# --- 1) 取 Brooklyn 多边形 ---\n",
    "url = \"https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=pgeojson\"\n",
    "features = requests.get(url).json()[\"features\"]\n",
    "bk = shape(next(f[\"geometry\"] for f in features if f[\"properties\"][\"BoroName\"] == \"Brooklyn\"))\n",
    "\n",
    "# --- 2) bbox 粗筛（全是 float 之后就不会再 TypeError）---\n",
    "# coarse filtering, find the bounding max(long,lat) of Brooklyn\n",
    "minx, miny, maxx, maxy = bk.bounds\n",
    "\n",
    "start_bbox = df[\"start_lng\"].between(minx, maxx) & df[\"start_lat\"].between(miny, maxy)\n",
    "end_bbox   = df[\"end_lng\"].between(minx, maxx)   & df[\"end_lat\"].between(miny, maxy)\n",
    "bbox_mask  = start_bbox | end_bbox\n",
    "\n",
    "# --- 3) 精筛（Shapely 2.x 向量化；如果你是 1.x，可改用 GeoPandas）---\n",
    "# fine filtering using Shapely\n",
    "cand = df[bbox_mask]\n",
    "\n",
    "# As we literally have millions of pair 2D coordinates, I learned online to use shapely's vectorized operations storing points\n",
    "p_start = shapely.points(cand[\"start_lng\"].to_numpy(), cand[\"start_lat\"].to_numpy())\n",
    "p_end   = shapely.points(cand[\"end_lng\"].to_numpy(),   cand[\"end_lat\"].to_numpy())\n",
    "\n",
    "# 点在多边形内；若希望“边界也算在内”，用 shapely.covers(bk, p_start/p_end)\n",
    "# contain would create a mask for points that are strictly inside the polygon\n",
    "# result in a boolean array\n",
    "in_bk_start = shapely.contains(bk, p_start)\n",
    "in_bk_end   = shapely.contains(bk, p_end)\n",
    "keep_cand   = np.logical_or(in_bk_start, in_bk_end) \n",
    "\n",
    "#CAUTION: LENGTH OF keep_cand HERE MAY NOT MATCH THE ORIGINAL df!!!!!!\n",
    "#KEEP_CAND ONLY WRITES ON WHATEVER bbox_mask IS TRUE\n",
    "\n",
    "# create a final mask to filter the original DataFrame, assume all will toss out\n",
    "final_mask = np.zeros(len(df), dtype=bool)\n",
    "# WE ONLY WANT TO FILL THE POSITIONS WHERE bbox_mask IS TRUE\n",
    "final_mask[np.where(bbox_mask)[0]] = keep_cand\n",
    "\n",
    "df_bk = df[final_mask].copy()\n",
    "print(\"After filtering, number of trips in Brooklyn:\", len(df_bk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Storage for citibike_all_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n",
      "PROJECT_ROOT: /Users/kevinjiang/bootcamp_Jianke_Jiang/project/notebooks\n",
      "DATA_DIR: /Users/kevinjiang/bootcamp_Jianke_Jiang/project/notebooks/data\n",
      ".env loaded (if present)\n",
      "pyarrow: 21.0.0\n",
      "pandas: 2.3.1\n",
      "RAW -> /Users/kevinjiang/bootcamp_Jianke_Jiang/project/data/raw\n",
      "PROC -> /Users/kevinjiang/bootcamp_Jianke_Jiang/project/data/processed\n",
      "Processed data saved to /Users/kevinjiang/bootcamp_Jianke_Jiang/project/data/processed/Trips_Processed_20250820-230826.csv\n",
      "Parquet engine not available. Install pyarrow or fastparquet to complete this step.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import os, pathlib, datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "from src.config import load_env, get_key\n",
    "load_env()\n",
    "\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pandas as pd\n",
    "    print(\"pyarrow:\", pa.__version__)\n",
    "    print(\"pandas:\", pd.__version__)\n",
    "except Exception as e:\n",
    "    print(\"Import failed:\", repr(e))\n",
    "\n",
    "RAW = pathlib.Path(\"..\") / \"data/raw\"; RAW.mkdir(parents=True, exist_ok=True)\n",
    "PROC = pathlib.Path(\"..\") / \"data/processed\"; PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def ts(): return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "print('RAW ->', RAW.resolve())\n",
    "print('PROC ->', PROC.resolve())\n",
    "\n",
    "output_file = PROC / f\"Trips_Processed_{ts()}.csv\"\n",
    "df_bk.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_file.resolve()}\")\n",
    "\n",
    "# Save as Parquet\n",
    "pq_path = PROC / f\"Trips_Processed_{ts()}.parquet\"\n",
    "try:\n",
    "    df_bk.to_parquet(pq_path, engine=\"fastparquet\")\n",
    "except Exception as e:\n",
    "    print('Parquet engine not available. Install pyarrow or fastparquet to complete this step.')\n",
    "    pq_path = None\n",
    "pq_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the data for station Lawrence and Willougghby St.(closest to tandon for analysis, data way to big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7d/x7l36_js44n_hzx_nffjmj440000gn/T/ipykernel_3184/2348588444.py:6: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_bk = pd.read_csv(\"/Users/kevinjiang/bootcamp_Jianke_Jiang/project/data/processed/Trips_Processed_20250820-230826.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import datetime as dt  # <-- add this\n",
    "\n",
    "# Read CSV\n",
    "df_bk = pd.read_csv(\"/Users/kevinjiang/bootcamp_Jianke_Jiang/project/data/processed/Trips_Processed_20250820-230826.csv\")\n",
    "\n",
    "# Create timestamp function\n",
    "def ts():\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "PROC = pathlib.Path(\"..\") / \"data/processed\"\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create mask\n",
    "tandon_mask = (\n",
    "    (df_bk['start_station_name'] == 'Lawrence St & Willoughby St') |\n",
    "    (df_bk['end_station_name'] == 'Lawrence St & Willoughby St')\n",
    ")\n",
    "\n",
    "# Filter\n",
    "df_bk = df_bk[tandon_mask].copy()\n",
    "print(\"head \")\n",
    "\n",
    "# Output file\n",
    "output_file = PROC / f\"Trips_Tandon_Processed_{ts()}.csv\"\n",
    "df_bk.to_csv(output_file, index=False)\n",
    "\n",
    "def ts():\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "PROC = pathlib.Path(\"..\") / \"data/processed\"\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "output_file = PROC / f\"Trips_Tandon_EDA_{ts()}.csv\"\n",
    "df.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bootcamp_env]",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
